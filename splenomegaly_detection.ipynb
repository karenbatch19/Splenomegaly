{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ensemble model for training on data comprised of electronic radiologist notes to predict the presence of metastatic cancers. Model written in Python 3.\n",
    "\n",
    "**NOTE: the model presented here was created, trained, tested, and validated on data that is not available for public use. The code will not run as is. This code has been cleaned of any Protected Health Information and can be viewed as a means of inspecting the architecture of the model.**\n",
    "\n",
    "Training data is accepted in csv form. The csv is required to have a column of notes for a specified location (i.e. \"spleen\") for the x data and an associated column of metastases (i.e. \"spleen_metastases\") comprised of either Yes/No or Yes/Indeterminate/No values for the y data (0/1/2 is also accepted). An \"impression\" column is also required. These columns can be have any name, so long as those names are identified in the \"Setup\" portion of the code. So long as the data has these three columns the model should work. Any other columns will not disrupt prediction or be used at all.\n",
    "\n",
    "Libraries you'll need:\n",
    "- pandas\n",
    "- keras\n",
    "- numpy\n",
    "- matplotlib\n",
    "- sklearn\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the names of the columns you'll be using to train the model.\n",
    "# Later on, the Impression section will be added to the x-data, but since that is static,\n",
    "# we don't need to specify it.\n",
    "# For paths, specify from the root directory, i.e. /Users/name/documents/...,\n",
    "x_column_name = \"\" #i.e. \"BONES_SOFT_TISSUES\"\n",
    "y_column_name = \"\" #i.e. \"bones_metastases\"\n",
    "impression_column_name = \"\" #i.e. \"IMPRESSION\"\n",
    "training_csv_path = \"\"\n",
    "prediction_csv_path = \"\"\n",
    "export_csv_path = \"\"\n",
    "\n",
    "# Prediction styles seem to come in two forms: (1) Yes/Indeterminate/No, and (2) Yes/No.\n",
    "# The following variable is to account for the case where data is received in form (1), \n",
    "# but the goal is to predict for form (2) by setting all Indeterminate values to No.\n",
    "# If binary_prediction = True: Predictions will be made on the Yes/No scale.\n",
    "# If binary_prediction = False: Predictions will be made on the Yes/Indeterminate/No scale [input data-willing, of course].\n",
    "binary_prediction = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings Displayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_mapping(le):\n",
    "    '''\n",
    "    Return a dict mapping labels to their integer values from an SKlearn LabelEncoder.\n",
    "    This is literally just to be able to reference what target values are \n",
    "    mapped to what integers when label encoding. It is only used for clarity,\n",
    "    sanity checking, and labelling graphs.\n",
    "    \n",
    "    Args:\n",
    "        le: a fitted SKlearn LabelEncoder\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary showing applied mappings (i.e. 0:\"No\").\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({cl:le.transform([cl])[0]})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    \"\"\"\n",
    "    Function standardizes all text in one column of an identified dataframe with string manipulation.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to be edited.\n",
    "        text_field (str): Name of column to be edited.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The edited DataFrame.\n",
    "    \"\"\"\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\d\\.\\s\", \" \")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_LSA(test_data, test_labels, savepath=\"PCA.csv\", plot=True):\n",
    "#     \"\"\"\n",
    "#     Function to plot test data and labels using principal component analysis.\n",
    "#     Not used anywhere within this file.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): DataFrame to be edited.\n",
    "#         text_field (str): Name of column to be edited.\n",
    "\n",
    "#     Returns:\n",
    "#         DataFrame: The edited DataFrame.\n",
    "#     \"\"\"\n",
    "#     lsa = TruncatedSVD(n_components=2)\n",
    "#     lsa.fit(test_data)\n",
    "#     lsa_scores = lsa.transform(test_data)\n",
    "#     color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "#     color_column = [color_mapper[label] for label in test_labels]\n",
    "#     colors = ['red', 'blue', 'green', 'yellow', 'black']\n",
    "\n",
    "#     red_patch = mpatches.Patch(color='red',label='init')\n",
    "#     blue_patch = mpatches.Patch(color='blue',label='init')\n",
    "#     green_patch = mpatches.Patch(color='green',label='init')\n",
    "#     yellow_patch = mpatches.Patch(color='yellow',label='init')\n",
    "#     black_patch = mpatches.Patch(color='blue',label='init')\n",
    "\n",
    "#     patches = [red_patch, blue_patch, green_patch, yellow_patch, black_patch]\n",
    "\n",
    "#     if plot:\n",
    "#         plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#         count = 0\n",
    "#         for key,val in chosenMappings.items():\n",
    "#             patches[count] = mpatches.Patch(color=colors[count], label=key)\n",
    "#             count+=1\n",
    "\n",
    "#         plt.legend(handles=patches[0:len(chosenMappings)], prop={'size': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Metric Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, y_predicted, binary_prediction): \n",
    "    \"\"\"\n",
    "    Function calculates accuracy metrics for a given set of predictions.\n",
    "\n",
    "    Args:\n",
    "        y_test (list): Target data truth.\n",
    "        y_predicted (list): Target data predictions.\n",
    "        binary_prediction (bool): whether or not you're making a binary prediction (assigned in setup)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The edited DataFrame.\n",
    "    \"\"\"\n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, \n",
    "                              y_predicted)\n",
    "        \n",
    "    if binary_prediction == True:\n",
    "        # true positives / (true positives+false positives)\n",
    "        precision = precision_score(y_test, \n",
    "                                    y_predicted, \n",
    "                                    average=\"binary\", \n",
    "                                    pos_label=1) \n",
    "\n",
    "        # true positives / (true positives + false negatives)\n",
    "        recall = recall_score(y_test, \n",
    "                              y_predicted, \n",
    "                              average=\"binary\", \n",
    "                              pos_label=1)\n",
    "\n",
    "        # harmonic mean of precision and recall\n",
    "        f1 = f1_score(y_test, \n",
    "                      y_predicted, \n",
    "                      average=\"binary\", \n",
    "                      pos_label=1)\n",
    "    \n",
    "    else:\n",
    "        # true positives / (true positives+false positives)\n",
    "        precision = precision_score(y_test, \n",
    "                                    y_predicted, \n",
    "                                    pos_label=None,\n",
    "                                    average='weighted') \n",
    "\n",
    "        # true positives / (true positives + false negatives)\n",
    "        recall = recall_score(y_test, \n",
    "                              y_predicted, \n",
    "                              pos_label=None,\n",
    "                              average='weighted')\n",
    "\n",
    "        # harmonic mean of precision and recall\n",
    "        f1 = f1_score(y_test, \n",
    "                      y_predicted, \n",
    "                      pos_label=None, \n",
    "                      average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    \"\"\"\n",
    "    Function generates a confusion matrix.\n",
    "    Not currently used in this file.\n",
    "\n",
    "    Args:\n",
    "        cm (): \n",
    "        classes (): \n",
    "        normalize (bool): whether or not to normalize the data. Default is false.\n",
    "        title (string): Title to be displayed.\n",
    "        cmap (???): Colour mapping.\n",
    "\n",
    "    Returns:\n",
    "        plt: The confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, origin='lower')\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    \"\"\"\n",
    "    Function creates a tfidf tokenized training set and associated vectorizer for test data.\n",
    "\n",
    "    Args:\n",
    "        data (list): Training data to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        list: The edited training data.\n",
    "        tfidf_vectorizer: tfidf vectorizer to be applied to test data.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(training_csv_path)\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings: \n",
      "{0: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "# Label encode the data.\n",
    "le = LabelEncoder()\n",
    "mappingsList = []\n",
    "\n",
    "le.fit(df_input[y_column_name])\n",
    "integerMapping = get_integer_mapping(le)\n",
    "mappingsList.append(integerMapping) # This is only used for graphs and viz later, if those things are included.\n",
    "df_input[y_column_name] = le.transform(df_input[y_column_name])\n",
    "\n",
    "print(\"Mappings: \\n\" + str(integerMapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3920\n",
      "1     182\n",
      "Name: bones_metastases, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "# Convert Indeterminate values to No for binary prediction\n",
    "# Contingent on the binary_prediction variable being set to True in Setup.\n",
    "if binary_prediction == True:\n",
    "    df_input.loc[df_input[y_column_name] == 'Indeterminate', y_column_name] = 'No'\n",
    "    \n",
    "print(df_input[y_column_name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing both the location notes and the impression notes. Will combine them shortly.\n",
    "df_input = standardize_text(df_input, x_column_name) \n",
    "df_input = standardize_text(df_input, impression_column_name)\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list out of the labels from the dataframe for use in creating train and test sets.\n",
    "chosen_labels_for_training = df_input[y_column_name].tolist()\n",
    "chosen_labels_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list out of the note data (both location and impression) for use in creating train and test sets.\n",
    "list_corpus = (df_input[x_column_name] + df_input[impression_column_name]).tolist()\n",
    "list_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 3281\n",
      "X_test length: 821\n",
      "y_train length: 3281\n",
      "y_test length: 821\n"
     ]
    }
   ],
   "source": [
    "# Creating train and test sets. First two variables there are your x and y.\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, \n",
    "                                                    chosen_labels_for_training, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=40)\n",
    "\n",
    "print(f\"X_train length: {len(X_train)}\\nX_test length: {len(X_test)}\\ny_train length: {len(y_train)}\\ny_test length: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF tokenization.\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# x_pred_tfidf = tfidf_vectorizer.transform(df_mets_targets.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "\n",
    "#parameters:\n",
    "#C selected through iterative testing, changed from the time above (lower worked better this time)\n",
    "#weight balanced, all yvalues considered equally\n",
    "#newton algorithm used to solve, best for multinomial data\n",
    "#n_jobs, -1 means use all processors\n",
    "clf_lr = LogisticRegression(C=15.0, \n",
    "                               class_weight='balanced', \n",
    "                               solver='newton-cg', \n",
    "                               multi_class='multinomial', \n",
    "                               n_jobs=-1, \n",
    "                               random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf_svm = svm.SVC(kernel='linear',\n",
    "                  probability=True) # Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create the model with 100 trees\n",
    "clf_rf = RandomForestClassifier(n_estimators=2000, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "clf_xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr \n",
    "clf_lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test_tfidf)\n",
    "accuracy_lr, precision_lr, recall_lr, f1_lr = get_metrics(y_test, y_pred_lr, binary_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "clf_svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_test_tfidf)\n",
    "accuracy_svm, precision_svm, recall_svm, f1_svm = get_metrics(y_test, y_pred_svm, binary_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "clf_rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_test_tfidf)\n",
    "\n",
    "# # I feel like the code below was important for something. Something about multiple columns maybe?\n",
    "# rf_probs = clf_rf.predict_proba(X_test_tfidf)[:, 1]\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_value = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf = get_metrics(y_test, y_pred_rf, binary_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb\n",
    "clf_xgb.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = clf_xgb.predict(X_test_tfidf)\n",
    "accuracy_xgb, precision_xgb, recall_xgb, f1_xgb = get_metrics(y_test, y_pred_xgb, binary_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated weights: \n",
      "[28.9945720472517, 52.86306382073741, 18.0680983314966, 22.217122943371407]\n"
     ]
    }
   ],
   "source": [
    "# Calculating weightings for how to appropriately weight performance of models in ensemble voting.\n",
    "avgAcc = (accuracy_lr+accuracy_svm+accuracy_rf+accuracy_xgb)/4\n",
    "avgPre = ((precision_lr+precision_svm+precision_rf+precision_xgb)/4)\n",
    "avgRec = (recall_lr+recall_svm+recall_rf+recall_xgb)/4\n",
    "sumAverages = avgAcc + avgPre + avgRec\n",
    "\n",
    "orderedWeights = [((accuracy_lr+(precision_lr*1.25)+recall_lr-sumAverages)*100)+10, \n",
    "                  ((accuracy_svm+(precision_svm*1.25)+recall_svm-sumAverages)*100)+10, \n",
    "                  ((accuracy_rf+(precision_rf*1.25)+recall_rf-sumAverages)*100)+10, \n",
    "                  ((accuracy_xgb+(precision_xgb*1.25)+recall_xgb-sumAverages)*100)+10]\n",
    "\n",
    "print(\"Calculated weights: \\n\" + str(orderedWeights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Ensemble Voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier - Soft\n",
    "ensemble_soft=VotingClassifier(estimators=[('Regression', clf_lr), \n",
    "                                           ('SVM', clf_svm), \n",
    "                                           ('RF', clf_rf), \n",
    "                                           ('XGB', clf_xgb)], \n",
    "                               voting='soft',\n",
    "                               weights=orderedWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Ensemble Voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble_soft\n",
    "ensemble_soft.fit(X_train_tfidf, y_train)\n",
    "y_pred_ensemble_soft = ensemble_soft.predict(X_test_tfidf)\n",
    "accuracy_ensemble_soft, precision_ensemble_soft, recall_ensemble_soft, f1_ensemble_soft = get_metrics(y_test, y_pred_ensemble_soft, binary_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-S accuracy = 0.976, precision = 0.808, recall = 0.583, f1 = 0.677\n",
      "\n",
      "Built on...\n",
      "LRe accuracy = 0.963, precision = 0.571, recall = 0.667, f1 = 0.615\n",
      "SVM accuracy = 0.972, precision = 1.000, recall = 0.361, f1 = 0.531\n",
      "RFo accuracy = 0.957, precision = 1.000, recall = 0.028, f1 = 0.054\n",
      "XGB accuracy = 0.967, precision = 0.714, recall = 0.417, f1 = 0.526\n"
     ]
    }
   ],
   "source": [
    "print(\"E-S accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_ensemble_soft, precision_ensemble_soft, recall_ensemble_soft, f1_ensemble_soft))\n",
    "print(\"\\nBuilt on...\")\n",
    "print(\"LRe accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_lr, precision_lr, recall_lr, f1_lr))\n",
    "print(\"SVM accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_svm, precision_svm, recall_svm, f1_svm))\n",
    "print(\"RFo accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_rf, precision_rf, recall_rf, f1_rf))\n",
    "print(\"XGB accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_xgb, precision_xgb, recall_xgb, f1_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Unannotated Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import based on location identified in setup\n",
    "df_unannotated = pd.read_csv(prediction_csv_path)\n",
    "\n",
    "# Standardize text\n",
    "df_unannotated = standardize_text(df_unannotated, x_column_name) \n",
    "df_unannotated = standardize_text(df_unannotated, impression_column_name)\n",
    "\n",
    "# Create combined impression + location corpus\n",
    "df_unannotated[\"Corpus\"] = df_unannotated[x_column_name] + df_unannotated[impression_column_name]\n",
    "\n",
    "# Create tfidf vectorized data\n",
    "x_pred_tfidf = tfidf_vectorizer.transform(df_unannotated[\"Corpus\"])\n",
    "\n",
    "# Make predictions \n",
    "predicted = ensemble_soft.predict(x_pred_tfidf)\n",
    "\n",
    "# Add predictions as column to dataframe\n",
    "df_unannotated[\"Predicted\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe\n",
    "df_unannotated.to_csv(export_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
